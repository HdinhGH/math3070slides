\documentclass[handout]{beamer}
%\documentclass{beamer}
%\documentclass{article}
\usepackage{animate}
\usepackage{amsmath}
\usepackage{array}
\usepackage{graphicx}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{caption}{\insertcaption} \setbeamertemplate{caption label separator}{}
\renewcommand{\emph}{\textbf}
\usetheme{Warsaw}
\title{Ch. 6 -- Point Estimation}
%\setlength{\parskip}{.2cm}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}

\begin{document}
\begin{frame}
\begin{beamercolorbox}[rounded=true,wd=\textwidth,center]{title}
\usebeamerfont{title}\inserttitle
\end{beamercolorbox}
\end{frame} 

\begin{frame}{Parameters and Estimators}
A \emph{parameter} is a constant describing a distribution:
\begin{itemize}
\item In a normal distribution, the mean $\mu$ and variance $\sigma^2$ are parameters.
\pause\item In an exponential distribution, the rate $\lambda$ is a parameter.
\pause\item We will often use the symbol $\theta$ to represent a parameter generically.
\end{itemize}

\pause An \emph{estimator} is a random variable which is used to estimate a parameter.
\begin{itemize}
\pause\item Given a random sample $X_1,\dots,X_n$, the sample mean $\overline{X} = \frac1n(X_1+\cdots+X_n)$ is an estimator for the mean $\mu$.
\pause\item The sample variance $S^2 = \frac1{n-1}\sum_{i=1}^n (X_i-\overline{X})^2$ is an estimator for the variance $\sigma^2$.
\pause\item We will often use the symbol $\hat\theta$ to represent an estimator generically.
\end{itemize}
\end{frame}


\begin{frame}{Unbiased Estimators}
\begin{block}{}
An estimator $\hat\theta$ is an \emph{unbiased} estimator for $\theta$ if $E(\hat\theta) = \theta$.
\end{block}
\pause\begin{itemize}
\item If $X$ is a binomial random variable, then the \emph{sample proportion} $\hat p=\frac X n$ is an unbiased estimator of $p$:
\pause $$E(\hat p) = E\left(\frac X n\right) 
\uncover<4->{= \frac1nE(X) }
\uncover<5->{= \frac1n\cdot np }
\uncover<6->{= p}$$
\item \uncover<7->{ If $X_1,\dots,X_n$ is a random sample from a distribution with mean $\mu$, the sample mean $\overline{X}$ is an unbiased estimator for $\mu$:}
\pause \begin{align*}
\uncover<8->{E(\overline{X}) &= E\left(\frac1n\sum_{i=1}^n X_i\right)}
\uncover<9->{= \frac1n E\left(\sum_{i=1}^n X_i\right) \\}
\uncover<10->{&= \frac1n \sum_{i=1}^n E(X_i) }
\uncover<11->{= \frac1n \sum_{i=1}^n \mu }
\uncover<12->{= \frac1n \cdot n\mu}
\uncover<13->{= \mu}
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Example: Uniform Distribution}
Given a random sample $X_1,\dots,X_n$ from a uniform distribution on $[0,\theta]$, how do we estimate $\theta$?

\begin{itemize}
\pause\item The mean $\mu$ is the midpoint of the interval, $\mu=\theta/2$. Therefore, $\theta=2\mu$. Since we can estimate $\mu$ with $\overline{X}$, we can estimate $\theta$ with $\hat\theta = 2\overline{X}$.
\pause\item Each of the observations $X_1,\dots,X_n$ will be less than $\theta$, and if $n$ is large we expect one of them to be close to $\theta$. So we may estimate $\theta$ using the maximum: $\hat\theta = \max\{X_1,\dots,X_n\}$.
\end{itemize}
\end{frame}

\begin{frame}{Example: Uniform Distribution}
We gave two possible estimators for the parameter $\theta$ of a uniform distribution on $[0,\theta]$:
\begin{align*}
\hat\theta_1 &= 2\overline{X} \\
\hat\theta_2 &= \max\{X_1,\dots,X_n\}
\end{align*}
\pause Question: Are these estimators unbiased?

\vspace{.2cm}
\pause We may calculate the expected value of $\hat\theta_1$:
 \begin{align*}
\uncover<4->{E(\hat\theta_1) = E(2\overline{X})}
\uncover<5->{= 2E(\overline{X}) }
\uncover<6->{= 2\mu }
\uncover<7->{= \theta}
\end{align*}
\uncover<8->{Since $E(\hat\theta_1)=\theta$, this means that $\hat\theta_1$ is an unbiased estimator for $\theta$.}
\end{frame}

\begin{frame}{Example: Uniform Distribution}
%Now we want to check if $\hat\theta_2=\max\{X_1,\dots,X_n\}$ is an unbiased estimator for the parameter $\theta$ of a uniform distribution $[0,\theta]$.

Since $\hat\theta_2=\max\{X_1,\dots,X_n\}$ is always less than $\theta$, intuition suggests that $\hat\theta_2$ will underestimate $\theta$ and hence must be biased.

\vspace{.2cm}
\pause Now we will calculate $E(\hat\theta_2)$. For $0\leq x\leq \theta$, the cdf $F(x)$ of $\hat\theta_2$ is
\begin{align*}
\uncover<2->{F(x) &= P(\hat\theta_2 \leq x) }
\uncover<3->{= P(\max\{X_1,\dots,X_n\} \leq x) \\}
\uncover<4->{&= P(X_1\leq x, X_2\leq x, \dots, X_n\leq x)\\}
\uncover<5->{&= P(X_1\leq x)P(X_2\leq x)\cdots P(X_n\leq x) = (x/\theta)^n}
\end{align*}

\uncover<6->{Therefore the pdf of $\hat\theta_2$ is, for $0\leq x\leq \theta$,}
$$\uncover<6->{f(x) = F'(x) }
\uncover<7->{= \frac{d}{dx} (x/\theta)^n }
\uncover<8->{= nx^{n-1}/\theta^n$$}
\uncover<9->{So the expected value of $\hat\theta_2$ is}
$$\uncover<9->{E(\hat\theta_2) = \int_{-\infty}^\infty xf(x)\ dx }
\uncover<10->{= \int_0^\theta x\cdot \frac{nx^{n-1}}{\theta^n}\ dx }
\uncover<11->{= \frac{n}{n+1}\theta$$}
%Since $E(\hat\theta_2) \neq \theta$, this means that $\hat\theta_2$ is a biased estimator of $\theta$.
\end{frame}

\begin{frame}{Example: Uniform Distribution}
We considered two estimators for the parameter $\theta$ of a uniform distribution on $[0,\theta]$,
\begin{align*}
\hat\theta_1 &= 2\overline{X} \\
\hat\theta_2 &= \max\{X_1,\dots,X_n\}
\end{align*}
\pause We found that $\hat\theta_1$ was unbiased but that $\hat\theta_2$ was biased. \pause However, it is easy to modify $\hat\theta_2$ to produce an unbiased estimator $\hat\theta_3$:
$$\hat\theta_3 = \frac{n+1}n \hat\theta_2 = \frac{n+1}n\max\{X_1,\dots,X_n\}$$
\pause Since $E(\hat\theta_2)=\frac n{n+1}\theta$, it follows that
$$E(\hat\theta_3) = E\left(\frac{n+1}n\hat\theta_2\right) 
\uncover<5->{= \frac{n+1}n E(\hat\theta_2) }
\uncover<6->{= \frac{n+1}n\cdot\frac n{n+1}\theta = \theta}$$
\uncover<7->{so $\hat\theta_3$ is in fact an unbiased estimator for $\theta$.}
\end{frame}

\begin{frame}{Example: Uniform Distribution}
We now have two unbiased estimators for the parameter $\theta$ of a uniform distribution on $[0,\theta]$:
\begin{align*}
\hat\theta_1 &= 2\overline{X} \\
\hat\theta_3 &= \frac{n+1}n\max\{X_1,\dots,X_n\}
\end{align*}
Question: Which of these estimators is better?

\pause \vspace{.2cm}To answer this, we need a measure of how good an estimator is. One commonly used such measure is the \textit{variance} of the estimator. 

\pause \vspace{.2cm}We can calculate the variance of $\hat\theta_1$:
\begin{align*}
V(\hat \theta_1) &= V(2\overline{X}) 
\uncover<4->{= 4V(\overline{X}) }
\uncover<5->{= \frac4nV(X_1) }
\uncover<6->{= \frac4n\cdot \frac{\theta^2}{12}}
\uncover<7->{ = \frac{\theta^2}{3n}}
\end{align*}
\end{frame}

\begin{frame}{Example: Uniform Distribution}
Recall the pdf of $\hat\theta_2$ is $f(x)=nx^{n-1}/\theta^n$. \pause Therefore,
\begin{align*}
\uncover<2->{E(\hat\theta_2^2) &= \int_0^\infty x^2f(x)\ dx }
\uncover<3->{= \int_0^\infty nx^{n+1}/\theta^n\ dx}
\uncover<4->{= \frac n{n+2}\theta^2\\}
\uncover<5->{V(\hat\theta_2) &= E(\hat\theta_2^2)-[E(\hat\theta_2)]^2}
\uncover<6->{= \frac{n}{n+2}\theta^2 - \left(\frac n{n+1}\theta\right)^2 \\}
\uncover<7->{&=\left(\frac{n(n+1)^2-n^2(n+2)}{(n+1)^2(n+2)}\right)\theta^2}
\uncover<8->{= \frac{n}{(n+1)^2(n+2)}\theta^2 \\}
\uncover<9->{V(\hat\theta_3) &= V\left(\frac{n+1}n \hat\theta_2\right) }
\uncover<10->{= \left(\frac{n+1}n\right)^2V(\hat\theta_2)\\}
\uncover<11->{&= \left(\frac{n+1}n\right)^2 \cdot \frac{n}{(n+1)^2(n+2)}\theta^2}
\uncover<12->{= \frac{\theta^2}{n(n+2)}}
\end{align*}
\end{frame}

\begin{frame}{Example: Uniform Distribution}
We considered two unbiased estimators for the parameter $\theta$ of a uniform distribution on $[0,\theta]$:
\begin{align*}
\hat\theta_1 &= 2\overline{X} \\
\hat\theta_3 &= \frac{n+1}n\max\{X_1,\dots,X_n\}
\end{align*}
\pause We calculated that their variances were
$V(\hat\theta_1)=\frac{\theta^2}{3n}$ and $V(\hat\theta_3)= \frac{\theta^2}{n(n+2)}$.
Therefore, for $n>1$, $\hat\theta_3$ has a smaller variance. 

\pause \vspace{.2cm} More advanced statistical theory can be used to show that in fact $\hat\theta_3$ is a \emph{minimum variance unbiased estimator}: it has a smaller variance than any other unbiased estimator. 
\end{frame}



\begin{frame}{Unbiasedness of Sample Variance}
\begin{block}{}
Given a distribution with variance $\sigma^2$,  the sample variance $S^2 = \frac1{n-1}\sum_{i=1}^n (X_i-\overline{X})^2$ is an unbiased estimator for $\sigma^2$.
\end{block}

\pause
\vspace{-.2cm}
\begin{align*}
E(S^2) &= E\left[\frac1{n-1}\left(\sum_{i=1}^n X_i^2 - n\overline{X}^2\right)\right] \\
\uncover<3->{&= \frac1{n-1}\left[\sum_{i=1}^n E(X_i^2)-nE(\overline{X}^2)\right]\\}
\uncover<4->{&= \frac1{n-1}\left[\sum_{i=1}^n (V(X_i)+[E(X_i)]^2)-n(V(\overline{X})+[E(\overline{X})]^2)\right]\\}
\uncover<5->{&= \frac1{n-1}\left[\sum_{i=1}^n (\sigma^2+\mu^2)-n(\frac{\sigma^2}n+\mu^2)\right]}
\uncover<6->{= \sigma^2}
%E(S^2) &= E\left[\frac1{n-1}\sum_{i=1}^n (X_i-\overline{X})^2\right] \\
%&= \frac1{n-1}E\left[\sum_{i=1}^n ((X_i-\mu)+(\mu-\overline{X}))^2\right]\\
%&= \frac1{n-1}E\left[\sum_{i=1}^n ((X_i-\mu)^2+2(X_i-\mu)(\mu-\overline{X})+(\mu-\overline{X})^2)\right]\\
\end{align*}
\end{frame}

% \begin{frame}{Minimum of Exponential Random Variables}
% \begin{block}{}
% Suppose $X_1,\dots,X_n$ are iid exponential random variables with mean $\mu$. Find an unbiased estimator for $\mu$ based on $\min\{X_1,\dots,X_n\}$.
% \end{block}
% \pause Solution: First we need to identify the distribution of $T=\min\{X_1,\dots,X_n\}$. \pause Letting $\lambda=\frac1\mu$, the cdf of $T$ is
% \begin{align*}
% F(t) &= P(T\leq t) = 1- P(T>t) \\
% \uncover<4->{&= 1-P(\min\{X_1,\dots,X_n\}>t) \\}
% \uncover<5->{&= 1-P(X_1>t, X_2>t,\dots, X_n>t)\\}
% %&= 1- P(X_1>t)P(X_2>t)\cdots P(X_n>t) \\
% \uncover<6->{&= 1 - (e^{-\lambda t})^n = 1-e^{-n\lambda t}}
% \end{align*}
% \uncover<7->{This is the cdf of an exponential random variable with rate $n\lambda$. }
% \uncover<8->{Now
% $$E(T) = \frac1{n\lambda} = \frac\mu n$$}
% \uncover<9->{So $\hat\mu=nT=n\min\{X_1,\dots,X_n\}$ is an unbiased estimator of $\mu$.}
% \end{frame}

% \begin{frame}{Minimum of Exponential Random Variables}
% Given a random sample $X_1,\dots,X_n$ from an exponential distribution with mean $\mu$, we found an unbiased estimator for $\mu$: 
% $$\hat\mu = nT = n\min\{X_1,\dots,X_n\}$$
% \pause \vspace{-.6cm}\begin{block}{}What is the variance of this estimator?
% \end{block}

% \vspace{.1cm}
% \pause Recalling that $T$ is exponential with rate $n\lambda$, we calculate
% \begin{align*}
% V(nT) = n^2V(T) = n^2\cdot \frac1{(n\lambda)^2} = \frac1{\lambda^2} = \mu^2
% \end{align*}
% \pause We note that as the sample size $n$ increases, the variance does not decrease but remains a constant $\mu^2$. This suggests that $\hat\mu$ is a poor estimator of $\mu$. \pause Compare this to the sample mean:

% $$V(\overline{X}) = \frac{V(X_1)}n=\frac{1/\lambda^2}n=\frac{\mu^2}n$$
% \end{frame}
%
%\begin{frame}{Censored Sampling}
%\begin{block}{}
%A company makes devices whose lifetimes are exponential with mean $\mu$. An engineer estimates $\mu$ by taking a random sample of 50 devices and measuring the lifetimes of the first 20 devices to fail. What is the expected amount of time the engineer must wait? 
%\end{block}
%\pause The waiting time $X_1$ for the first failure is the minimum of 50 independent exponential random variables each with rate $1/\mu$, so $X_1$ is exponential with rate $50/\mu$, which implies $E(X_1)=\mu/50$.
%
%\pause \vspace{.2cm}
%The additional waiting time $X_2$ for the next failure is the minimum of 49  exponential random variables each with rate $1/\mu$, so $X_2$ is exponential with rate $49/\mu$, which implies $E(X_2)=\mu/49$.
%
%\pause \vspace{.2cm}Continuing, we have $E(X_i)=\mu/(51-i)$, \pause so
%\begin{align*}
%E(X_1+X_2+\cdots+X_{20})&=E(X_1)+E(X_2)+\cdots+E(X_{20}) \\
%&= \frac\mu{50}+\frac\mu{49}+\frac\mu{48}+\cdots+\frac\mu{31}
%\approx .504\mu
%\end{align*}
%%
%%If $Y_i$ is the lifetime of the $i$th device, then the first failure occurs at $X_1=\min\{Y_1,\dots,Y_{40}\}$. Since each $Y_i$ is an exponential random variable with rate $1/\mu$, $X_1$ is exponential with rate $40/\mu$.
%\end{frame}
%
%\begin{frame}{Censored Sampling}
%In the previous problem, an engineer waited for 20 out of 50 devices to fail, and $X_i$ was the amount of time between the $(i-1)$st and $i$th failures. Let $Y_i$ be the lifetime of the $i$th device to fail, so $Y_i=\sum_{j=1}^i X_j$.
%\pause \begin{block}{}
%Find the expected value of the \textit{total accumulated lifetime} 
%
%\vspace{-.2cm}$$\textstyle T=\sum_{i=1}^{20}Y_i + 30Y_{20}$$
%\end{block}
%
%\pause \vspace{-.5cm}
%\begin{align*}
%E(T) &= E\left(\sum_{i=1}^{20}Y_i + 30Y_{20}\right)
%\uncover<4->{=E\left(\sum_{i=1}^{20}\sum_{j=1}^i X_j + 30\sum_{j=1}^{20} X_j\right) \\}
%\uncover<5->{&=E\left(\sum_{j=1}^{20}\left(\sum_{i=j}^{20} X_j+30 X_j\right)\right)}
%\uncover<6->{=E\left(\sum_{j=1}^{20}(51-j)X_j \right) \\}
%\uncover<7->{&=\sum_{j=1}^{20}(51-j)E(X_j) }
%\uncover<8->{=\sum_{j=1}^{20}(51-j)\frac\mu{51-j} }
%\uncover<9->{= 20\mu}
%\end{align*}
%\end{frame}
%
%\begin{frame}{Censored Sampling}
%\begin{block}{}Find an unbiased estimator for the mean lifetime $\mu$ based on the total accumulated lifetime $T$. What is its variance?\end{block}
%\pause We found that $E(T)=20\mu$. Therefore, $\hat\mu=\frac T{20}$ is an unbiased estimator of $\mu$. \pause We can calculate its variance:
%
%\vspace{-.3cm}
%\begin{align*}
%V(\hat\mu) &= V\left(\frac T{20}\right) 
%\uncover<4->{= \frac1{400}V(T) }
%\uncover<5->{= \frac1{400}V\left(\sum_{j=1}^{20} (51-j)X_j\right) \\}
%\uncover<6->{&=\frac1{400}\sum_{j=1}^{20}(51-j)^2V(X_j) }
%\uncover<7->{=\frac1{400}\sum_{j=1}^{20}(51-j)^2\frac{\mu^2}{(51-j)^2} \\}
%\uncover<8->{&= \frac1{400}\cdot 20\mu^2 = \frac{\mu^2}{20}}
%\end{align*}
%
%\uncover<9->{It can be shown that in fact $\hat\mu=T/20$ is the minimum variance unbiased estimator for $\mu$.}
%%Note: If we waited until all 50 devices failed (instead of just the first 20), then the variance of the estimator would be reduced to $\mu^2/50$. However this is at the cost of having to wait longer.
%\end{frame}

\begin{frame}{Maximum Likelihood Estimation}
\begin{block}{}
Suppose a Poisson process has unknown rate $\lambda$. We observe the process for 2 hours, and the number of events which occur is $X=40$. What value of $\lambda$ gives the largest probability $P(X=40)$?
\end{block}
\pause $X$ is a Poisson random variable with mean $2\lambda$, so
$$P(X=40) = \frac{e^{-2\lambda}(2\lambda)^{40}}{40!}$$
\pause The function $L(\lambda)=\frac{e^{-2\lambda}(2\lambda)^{40}}{40!}$ here is called the \emph{likelihood function}. \pause To maximize $L(\lambda)$, we take the derivative:
$$L'(\lambda) = 
\frac1{40!}(e^{-2\lambda}\cdot40(2\lambda)^{39}\cdot2-2e^{-2\lambda}(2\lambda)^{40})$$
\pause Setting this equal to zero gives $40-2\lambda=0$, so $\lambda=20$. This is called the \emph{maximum likelihood estimate} of $\lambda$.
\end{frame}

\begin{frame}{Maximum Likelihood Estimation}
In the previous problem, the likelihood function was given by
$$L(\lambda)=\frac{e^{-2\lambda}(2\lambda)^{40}}{40!}$$
\pause The \emph{log-likelihood function} is
$$\ell(\lambda) = \ln(L(\lambda)) = -2\lambda+40\ln(2\lambda)-\ln(40!)$$
\pause The value of $\lambda$ which maximizes $\ell(\lambda)$ will also maximize $L(\lambda)$. The log-likelihood function is often easier to differentiate than the likelihood function. 
\pause In this case,
$$\ell'(\lambda) = -2+\frac{40}\lambda$$
\pause Setting this equal to zero gives $\lambda=20$ as before.
\end{frame}
%
%\vspace{.2cm}
%The derivative is $l'(\lambda)=-2+\frac{40}\lambda$. Setting this equal to 0 and solving for $\lambda$ gives $\lambda=20$. This is the \emph{maximum likelihood estimate} of $\lambda$.
%\end{frame}
%%
\begin{frame}
The log-likelihood function was found to be
$$l(\lambda)=\ln(L(\lambda)) = -2\lambda+40\ln(2\lambda)-\ln(40!)$$
The maximum of log-likelihood function may be found by setting the derivative equal to zero:
\begin{align*}
l'(\lambda) = -2+\frac{40}{2\lambda}\cdot 2 = -2+\frac{40}{\lambda} = 0
\end{align*}
Solving for $\lambda$ gives $\lambda=20$. This is the \emph{maximum likelihood estimate} for $\lambda$.
\end{frame}
%%
\begin{frame}{Maximum Likelihood Estimation}
In general, given a random variable $X$ with pmf or pdf $f(x; \theta)$ depending on a parameter $\theta$, the \emph{maximum likelihood estimator} (mle) is the value $\hat\theta$ of $\theta$ that maximizes $f(X; \theta)$. 

\vspace{.2cm}
\pause More generally, given a random sample $X_1,\dots,X_n$, the mle is the value $\hat\theta$ that maximizes the joint pmf or joint pdf of $X_1,\dots,X_n$: $f(X_1,\dots,X_n; \theta)=f(X_1;\theta)f(X_2;\theta)\cdots f(X_n;\theta)$.

\pause \vspace{.2cm}Under fairly general conditions, the maximum likelihood estimator $\hat\theta$ satisfies some desirable statistical properties:
\begin{itemize}
\pause \item $\hat\theta$ exists and is unique.
\pause \item $\hat\theta$ is \textit{asymptotically unbiased}: for large sample sizes, it is practically an unbiased estimator. 
\pause \item $\hat\theta$ is \textit{asymptotically efficient}: for large sample sizes, it approximately achieves the smallest possible variance for an unbiased estimator.
\pause \item $\hat\theta$ is \textit{asymptotically normal}: for large sample sizes, it has approximately a normal distribution.
\end{itemize}
\end{frame}

\begin{frame}{Example}
\begin{block}{}
Given a random sample $X_1,\dots,X_n$ from an exponential distribution with rate $\lambda$, find the mle for $\lambda$.
\end{block}
\pause
The pdf of each $X_i$ is $f(x;\lambda)=\lambda e^{-\lambda x}$. 
\pause Since $X_1,\dots,X_n$ are independent, the likelihood function is
$$L(\lambda)=f(x_1,\dots,x_n;\lambda) = \prod_{i=1}^n \lambda e^{-\lambda x_i} = \lambda^n e^{-\lambda\sum_{i=1}^n x_i}$$
\pause The log-likelihood is therefore
$$\ell(\lambda) = n\ln\lambda-\lambda\sum_{i=1}^n x_i$$
\pause Setting the derivative equal to zero gives $\frac n\lambda-\sum_{i=1}^n x_i=0$. \pause Solving for $\lambda$, we find the mle: $$\hat\lambda = \frac n{\sum_{i=1}^n X_i} 
\uncover<7->{= 1/\overline{X}}$$
\end{frame}

\begin{frame}{Example}
\begin{block}{}
Given an observation of an exponential random variable $X$ with unknown rate $\lambda$, find the mle for $\lambda$.
\end{block}
\pause
The pdf of each $X$ is $f(x)=\lambda e^{-\lambda x}$. 
\pause The log-likelihood is therefore
$$\ell(\lambda) = \ln\lambda-\lambda x$$
\pause Setting the derivative equal to zero gives $\frac 1\lambda-x=0$. \pause Solving for $\lambda$, we find the mle: $$\hat\lambda = \frac 1X$$
\end{frame}

\begin{frame}{Invariance Principle for MLE}
\begin{block}{}
Let $h(\theta)$ be a function of $\theta$. If $\hat\theta$ is the mle for $\theta$, then $h(\hat\theta)$ is the mle for $h(\theta)$.
\end{block}
%\pause Example: Given a random sample $X_1,\dots,X_n$ from an exponential distribution with rate $\lambda$, we found that the mle for $\lambda$ was $$\hat\lambda = \frac{n}{\sum_{i=1}^n X_i}$$
%Therefore, the mle for $\mu=1/\lambda$ is $$\hat\mu = 1/\hat\lambda = \frac{\sum_{i=1}^n X_i}n = \overline{X}$$
%Thus, the mle for $\mu$ is simply the sample mean. It can be shown that this in fact is the minimum variance unbiased estimator for $\mu$.
%
\pause Example: Given an observation of an exponential random variable $X$ with unknown rate $\lambda$, we found that the mle for $\lambda$ was $$\hat\lambda = \frac1X$$
Therefore, the mle for $\mu=1/\lambda$ is $$\hat\mu = 1/\hat\lambda = \frac1{1/X}=X$$
Thus, the mle for $\mu$ is simply the observed value $X$. It can be shown that this in fact is the minimum variance unbiased estimator for $\mu$.
\end{frame}

\begin{frame}{MLE of Several Parameters}
There are many families of distributions where we would want to estimate two parameters at once. For example,
\begin{itemize}
\item For a normal distribution, we want to estimate $\mu$ and $\sigma$.
\item For a gamma distribution, we want to estimate $k$ and $\lambda$.
\item For a Weibull distribution, we want to estimate $\alpha$ and $\beta$.
\end{itemize}
\pause The definition of mle extends in an natural way to distributions with several parameters:
\pause \begin{block}{}
Given a random sample $X_1,\dots,X_n$ from a distribution with several parameters $\theta_1,\dots,\theta_m$, the mle of $(\theta_1,\dots,\theta_m)$ is the combination of values $(\hat\theta_1,\dots,\hat\theta_m)$ maximizing the joint pmf or joint pdf $f(x_1,\dots,x_n ; \theta_1,\dots,\theta_m)$ of $X_1,\dots,X_n$.
\end{block}
\end{frame}

%\begin{frame}{MLE for Normal Distribution}
%\begin{block}{}
%Given a random sample $X_1,\dots,X_n$ from a normal distribution with mean $\mu$ and standard deviation $\sigma$, find the mle for $\mu$ and $\sigma$.
%\end{block}
%\pause The pdf of each $X_i$ is $f(x)=\frac1{\sigma\sqrt{2\pi}}e^{-(x-\mu)^2/2\sigma^2}$, 
%\pause so the likelihood function is
%$$L(\mu,\sigma)=\prod_{i=1}^n \frac1{\sigma\sqrt{2\pi}}e^{-(x_i-\mu)^2/2\sigma^2}$$
%\pause The log-likelihood and its derivative with respect to $\mu$ are then
%\begin{align*}
%\ell(\mu,\sigma) &= n\ln\frac1{\sigma\sqrt{2\pi}}-\sum_{i=1}^n \frac{(x_i-\mu)^2}{2\sigma^2} \\
%\uncover<5->{\frac{\partial \ell}{\partial\mu} &= \sum_{i=1}^n\frac{x_i-\mu}{\sigma^2}}
%\end{align*}
%\uncover<6->{Setting $\frac{\partial\ell}{\partial\mu}=0$ and solving gives $\hat\mu=\frac1n\sum_{i=1}^n X_i=\overline{X}$.}
%\end{frame}
%
%\begin{frame}{MLE for Normal Distribution}
%Now we need to find the mle $\hat\sigma$. \pause We found the log-likelihood:
%$$\ell(\mu,\sigma) = n\ln\frac1{\sigma\sqrt{2\pi}}-\sum_{i=1}^n \frac{(x_i-\mu)^2}{2\sigma^2} $$
%\pause Taking the derivative with respect to $\sigma$ gives
%$$\frac{\partial\ell}{\partial\sigma} = -\frac n\sigma+\sum_{i=1}^n \frac{(x_i-\mu)^2}{\sigma^3}$$
%\pause Setting this equal to 0 gives the mle for $\sigma$:
% $$\hat\sigma = \sqrt{\frac1n\sum_{i=1}^n (X_i-\hat\mu)^2} = \sqrt{\frac1n\sum_{i=1}^n (X_i-\overline{X})^2}$$
%\pause Note the occurence of $n$ in the denominator rather than $n-1$. 
%\end{frame}
%
\begin{frame}{Estimating Parameters of a Normal Distribution}
Given a random sample $X_1,\dots, X_n$ from a normal distribution:
\begin{block}{}
The MLE for $\mu$ is the sample mean $\overline{X}$, and this is also the minimum variance unbiased estimator.
\end{block}
This means that alternative estimators for $\mu$, such as the sample median $\tilde X$, must have a greater variance. (In fact, $V(\overline{X})=\frac{\sigma^2}n$, while $V(\tilde X) \approx \frac\pi2\cdot \frac{\sigma^2}n$ for large $n$.)

\begin{block}{}
The MLE for $\sigma^2$ is $\hat{\sigma}^2=\frac1{n}\sum_{i=1}^n (X_i-\overline X)^2$. However, this estimator is biased; the minimum variance unbiased estimator is the sample variance $S^2=\frac1{n-1}\sum_{i=1}^n (X_i-\overline X)^2$.
\end{block}
%\end{itemize}

%\pause For example, an alternative estimator for the mean of a normal distribution is given by the sample median $\tilde X$. However, the sample median has a greater variance than that of the sample mean, $V(\overline{X})=\frac{\sigma^2}n$. (In fact, for large $n$, $V(\tilde X) \approx \frac\pi2\cdot \frac{\sigma^2}n$)
%
%\vspace{.2cm}
%\pause Thus, for normally distributed data, the sample mean $\overline{X}$ is a more efficient estimator than the sample median $\tilde X$ (or any other estimator).
%
%\vspace{.2cm}
%\pause However, it does not follow that the sample mean $\overline{X}$ is in all respects a better estimator than the sample median $\tilde X$. Recall that the sample median $\tilde X$ has the advantage of being a \emph{robust} estimator: it is relatively unaffected by a few bad data points. 
\end{frame}

%\begin{frame}{MLE of Weibull Distribution}
%\begin{block}{}Given a random sample $X_1,\dots,X_n$ from a Weibull distribution, find the mle for $\alpha$ and $\beta$.
%\end{block}\small
%\pause Recall the pdf of a Weibull is $f(x;\alpha,\beta) =  \frac{\alpha}{\beta^\alpha}x^{\alpha-1}e^{-(x/\beta)^\alpha}$. 
%\pause The likelihood function is therefore
%$$L(\alpha,\beta) = \prod_{i=1}^n \frac{\alpha}{\beta^\alpha}x_i^{\alpha-1}e^{-(x_i/\beta)^\alpha}
%=\frac{\alpha^n}{\beta^{n\alpha}}\left(\prod_{i=1}^n x_i\right)^{\alpha-1}e^{-\sum_{i=1}^n (x_i/\beta)^\alpha}$$
%\pause The log-likelihood is therefore
%
%\vspace{-.5cm}$$\ell(\alpha,\beta) = n\ln \alpha-n\alpha\ln\beta+(\alpha-1)\sum_{i=1}^n\ln x_i-\sum_{i=1}^n(x_i/\beta)^\alpha$$
%
%\pause Taking derivatives gives equations to be numerically solved for $\alpha$ and $\beta$:
%\begin{align*}
%\frac{\partial\ell}{\partial\alpha} &= \frac n\alpha-n\ln\beta+\sum_{i=1}^n\ln x_i-\sum_{i=1}^n (x_i/\beta)^{\alpha}\ln(x_i/\beta)=0\\
%\frac{\partial\ell}{\partial\beta} &= -\frac{n\alpha}{\beta}-\sum_{i=1}^n\alpha(x_i/\beta)^{\alpha-1}\cdot\frac{-x_i}{\beta^2}=0
%\end{align*}
%\end{frame}
%
%\begin{frame}{MLE of Uniform Distribution}
%\begin{block}{}Given a random sample $X_1,\dots,X_n$ from a uniform distribution on $[0,\theta]$, find the mle for $\theta$.
%\end{block}
%
%
%\vspace{.2cm}The pdf of each $X_i$ is $f(x;\theta) = \begin{cases}1/\theta, & 0\leq x\leq \theta \\ 0, & \text{otherwise}\end{cases}$
%
%\pause \vspace{.2cm}
%Therefore, the likelihood function is 
%$$L(\theta)=\begin{cases}1/\theta^n, & \text{if $x_1,\dots,x_n$ are all between 0 and $\theta$} \\ 0, & \text{otherwise}\end{cases}$$
%
%\pause Instead of using calculus to try to find the value of $\theta$ that maximizes $L(\theta)$, we simply notice that $L(\theta)$ is largest when $\theta$ is as small as possible while requiring $x_1,\dots,x_n$ to be between 0 and $\theta$. 
%
%\pause \vspace{.2cm}The smallest such value of $\theta$ is $\hat\theta=\max\{X_1,\dots,X_n\}$.
%\end{frame}
%
%\begin{frame}{Animal Tagging Revisited}
%\begin{block}{}Suppose a researcher tags $M$ animals of a certain species, then allows them to mix back into the population. If the researcher later randomly samples $n$ animals from this population and finds that $X$ of them are tagged, find the mle for the total population $N$.
%\end{block}
%\pause $X$ is a hypergeometric random variable, with pmf
%$$f(x; N) = \frac{\binom M x \binom {N-M}{n-x}}{\binom N n}$$
%\pause Since $N$ only takes integer values, it doesn't make sense to differentiate with respect to $N$. But we can evaluate the ratio
%\begin{align*}
%\frac{f(x;N)}{f(x;N-1)} = \frac{\binom{N-M}{n-x}\binom{N-1}n}{\binom{N-1-M}{n-x}\binom N n}
%\uncover<4->{= \frac{(N-M)(N-n)}{(N-M-n+x)N}}
%\end{align*}
%\uncover<5->{This ratio is at least 1 provided $N \leq \frac{Mn}x$, so the mle is $\hat N=\left\lfloor\frac{Mn}x\right\rfloor$.}
%\end{frame}

%\begin{frame}{Method of Moments}
%\end{frame}
%
%\begin{frame}{Estimating Parameter of Exponential Distribution}
%\end{frame}

% Maximum likelihood estimation
\end{document}