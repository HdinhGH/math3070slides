\documentclass[t,handout]{beamer}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usetikzlibrary{positioning,fit}
%\usepackage{enumitem}
\usetheme{Warsaw}
\setbeamertemplate{navigation symbols}{}
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\grn}[1]{{\color{green} #1}}
\newcommand{\bluRed}[2]{{\color{blue} #1}{\color{red} #2}}
\newcommand{\qtns}[0]{\begin{center} Questions? \end{center}}
\newcommand{\nl}[1]{\vspace{#1 em}}
\newcommand{\cntrImg}[2]{\begin{center}\includegraphics[scale=#2]{#1}\end{center}}
\newcommand{\defn}[1]{{\bf #1}}
\let\emptyset\varnothing
\newcommand{\SampS}[0]{$\mathcal{S}$}

\title{Math 3070, Applied Statistics}

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\begin{document}
\begin{frame}[c]
    \begin{beamercolorbox}[rounded=true,wd=\textwidth,center]{title}
        \usebeamerfont{title}\inserttitle
    \end{beamercolorbox}
    \begin{center}
        Section 1\\
        \nl{0.5}
        October 18, 2019
    \end{center}
\end{frame}
\begin{frame}[c]{Lecture Outline, 10/18}
    Section 5.5
    \begin{itemize}
        \item Linear Combinations
    \end{itemize}
\end{frame}
\begin{frame}[c]{Linear Combination, Definition}
    \begin{block}{}
        Consider $X_1,X_2, \ldots, X_n$ random variables and $a_1,a_2,\ldots, a_n$.
        $$Y = a_1 X_1+ a_2 X_2 + \ldots + a_n X_n$$
        is a \textbf{linear combination} of the $X_i$'s.
    \end{block}
    For example, setting $a_i =1/n$ yields the mean.
\end{frame}
\begin{frame}[c]{Linear Combination, Properties}
    \begin{block}{}
        Random variables $X_1,X_2, \ldots, X_n$ have means $\mu_1,\mu_2,\ldots, \mu_n$ and variances $\sigma^2_1,\sigma^2_2,\ldots,\sigma^2_n$, respectively.
        \begin{itemize}
            \item It is always true that 
            $$E(Y) = a_1 \mu_1 + \ldots + a_n \mu_n$$
            \item Assuming the $X_i$'s are independent
            $$Var(Y) = a_1^2 \sigma^2_1 + \ldots + a_n^2 \sigma^2_n$$
            \item It is always true that 
            $$ Var(Y) = \sum_{i=1}^n \sum_{j=1}^n a_i a_j Cov(X_i,X_j) $$
        \end{itemize}
    \end{block}
\end{frame}
\begin{frame}{Linear Combination, Properties}
    \begin{align*}
        E(Y) & = E(a_1 X_1+ a_2 X_2 + \ldots + a_n X_n)\\
        & = a_1 E[X_1]+ a_2 E[X_2] + \ldots + a_n E[X_n]\\
        & = a_1 \mu_1+ a_2 \mu_2 + \ldots + a_n \mu_n\\
        Var(Y) & = E\bigg[ \bigg( \sum_{i=1}^n a_i X_i - E\bigg[ \sum_{j=1}^n a_j X_j\bigg] \bigg)^2 \bigg]\\
        \uncover<2->{& = E\bigg[ \bigg( \sum_{i=1}^n a_i X_i -  \sum_{j=1}^n a_j E[X_j] \bigg)^2 \bigg]\\
        & = E\bigg[ \bigg( \sum_{i=1}^n \big(a_i X_i -   a_i E[X_i]\big) \bigg)^2 \bigg]\\}
        \uncover<3->{& = E\bigg[ \bigg( \sum_{i=1}^n \big(a_i X_i -   a_i E[X_i]\big) \bigg) \bigg( \sum_{j=1}^n \big(a_j X_j -   a_j E[X_j]\big) \bigg)\bigg]\\}
    \end{align*}
\end{frame}
\begin{frame}{Linear Combination, Properties}
    \begin{align*}
        & = E\bigg[ \sum_{i=1}^n \sum_{j=1}^n \big(a_i X_i -   a_i E[X_i]\big)  \big(a_j X_j -   a_j E[X_j]\big) \bigg]\\
        & =  \sum_{i=1}^n \sum_{j=1}^n E\bigg[ \big( a_i X_i -   a_i E[X_i]\big) \big(a_j X_j -   a_j E[X_j]\big) \bigg]\\
        & =  \sum_{i=1}^n \sum_{j=1}^n Cov(a_i X_i, a_j X_j)\\
        & =  \sum_{i=1}^n \sum_{j=1}^n a_i a_jCov(X_i, X_j)\\
        & \text{independence} \rightarrow Cov(X_i,X_j) =0 \text{ if } i\neq j\\
        Var(Y)& = \sum_{i=1}^n a_i^2 Cov(X_i,X_i) = \sum_{i=1}^n a_i^2 Var(X_i) \text{ , assuming independence}
    \end{align*}
\end{frame}
\begin{frame}{Linear Cominations of Normal Random Variables}
    If $X_i \sim N(\mu_i,\sigma_i)$, then $$Y=\sum_{i=1}^n a_i X_i \sim N\bigg(\sum_{i=1}^n a_i \mu_i, \sqrt{\sum_{i=1}^n a_i^2 \sigma^2_i} \bigg)$$
    Using variance notation instead of standard deviation notation looks more tidy.
    $$X_i \sim N(\mu_i,\sigma^2_i) \Rightarrow Y= \sum_{i=1}^n a_i X_i \sim N\bigg(\sum_{i=1}^n a_i \mu_i, \sum_{i=1}^n a_i^2 \sigma^2_i \bigg)$$
    I plan to use variance notation in the notes from here and onward.
\end{frame}
\begin{frame}{Linear Combination, Example}
    The number of bus arrivals at any stop are independent follow a Poisson distribution and average 5 hours an hour. You need to take 3 buses to reach Lagoon. Each bus ride takes 10 minutes. You are currently waiting for the first bus. Compute the mean and standard deviation of the total time of the trip.
    \uncover<2->{$$X_i = \# \text{ of buses in an hour at stop } i, X_i \sim Poisson(5)$$}
    \uncover<3->{$$T_i = \# \text{ waiting time at bus stop } i, T_i \sim \exp(5)$$}
    \uncover<4->{Due to the memoryless property, it does not matter when you arrive at a bus stop. The next wait time has the same distribution. Not true in reality.}
    \uncover<5->{\begin{align*}
        T &= \text{Total trip time} \\
          &= T_1 + 1/6 + T_2 + 1/6 + T_3 + 1/6\\
            & = T_1 + T_2 + T_3 + 1/2
    \end{align*}}
\end{frame}
\begin{frame}{Linear Combination, Example}
    \vskip -1em
    {\small
    \begin{align*}
        \uncover<2->{E[T] &=E[T_1 + T_2 + T_3 + 1/2]  \\
        &=E[T_1] + E[T_2] + E[T_3] + \frac{1}{2}  \\}
        \uncover<3->{&=\frac{1}{\lambda} + \frac{1}{\lambda} + \frac{1}{\lambda} + \frac{1}{2}  \\
        &=\frac{1}{5} + \frac{1}{5} + \frac{1}{5} + \frac{1}{2}  = \frac{11}{10}\\
        \uncover<4->{& = \text{an hour and ten minutes}\\}
        Var(T) & = V(T_1 + T_2 + T_3 + \frac{1}{2})\\
        & = V(T_1) + V(T_2) + V(T_3)\\}
        \uncover<5->{& = \frac{1}{\lambda^2} + \frac{1}{\lambda^2} + \frac{1}{\lambda^2} = \frac{1}{25} + \frac{1}{25} + \frac{1}{25}\\
        & \approx 7.2 \text{minutes} }\\
        \sigma & = \frac{\sqrt{3}}{5}
    \end{align*}
    }
\end{frame}
\begin{frame}{Example, Difference of Random Variables}
    A machine produces $X$ grams of chemical X. Another produces $Y$ grams of another chemical Y. Every two grams of chemical Y reacts one gram of chemical X when mixed. Assume that $X$ is normally distributed with a mean of 150 grams and a standard deviation of 2 grams, $Y$ is normally distributed with a mean of 300 grams and variance of 1 gram, and that $X$ and $Y$ are independent. What is the probability that more than one gram of chemical Y remains after mixing?
    \uncover<2->{$$ X\sim N(150,4) , Y\sim  N(300,1)$$}
    \uncover<3->{$L = \frac{1}{2}Y-X$ normally distributed. Just need the mean and standard deviation. There is more than one leftover gram of chemical Y when $L>0$.}
    {\small
    \uncover<4->{$$E[L] = \frac{1}{2}E[Y] - E[X] = \frac{1}{2}300 - 150 = 0 $$}
    \uncover<5->{$$ Var(L) = \bigg(\frac{1}{2}\bigg)^2Var(Y) + (-1)^2Var(X) = \frac{1}{4}+ 4=\frac{15}{4} $$}
    }
\end{frame}
\begin{frame}{Example, Difference of Random Variables}
    $$L\sim N\bigg(0,\frac{15}{4}\bigg)$$
    \uncover<2->{Want $P(L>1)$ which has an event contained inside the event that there is leftover chemical Y, $\{L>0\}$.}
    \begin{align*}
        \uncover<3->{P(L>1) & = P\bigg( \frac{L}{\sqrt{15/4}} > \frac{1}{\sqrt{15/4}} \bigg)\\
            \uncover<4->{& = P\bigg( Z > \frac{1}{\sqrt{15/4}} \bigg)\\
            & = \Phi(1/\sqrt{15/4}) \approx 0.3028}}
    \end{align*}
\end{frame}
\begin{frame}[c]{Summary}
    \begin{itemize}
        \item $Y = a_1 X_1+ a_2 X_2 + \ldots + a_n X_n$
        \item $ E[Y] = \sum_{i=1}^n a_i E[X_i]$
        \item Assuming independence, $Var(Y) = \sum_{i=1}^n a_i^2 Var(X_i) $
        \item Generally, $Var(Y) = \sum_{i=1}^n\sum_{j=1}^n a_i a_j Cov(X_i,X_j) $
        \item $X_i \sim N(\mu_i,\sigma^2_i) \Rightarrow$ \\
        $Y= \sum_{i=1}^n a_i X_i \sim N\bigg(\sum_{i=1}^n a_i \mu_i, \sum_{i=1}^n a_i^2 \sigma^2_i \bigg)$
    \end{itemize}
\end{frame}
\end{document}