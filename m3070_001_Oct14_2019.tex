\documentclass[t,handout]{beamer}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usetikzlibrary{positioning,fit}
%\usepackage{enumitem}
\usetheme{Warsaw}
\setbeamertemplate{navigation symbols}{}
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\grn}[1]{{\color{green} #1}}
\newcommand{\bluRed}[2]{{\color{blue} #1}{\color{red} #2}}
\newcommand{\qtns}[0]{\begin{center} Questions? \end{center}}
\newcommand{\nl}[1]{\vspace{#1 em}}
\newcommand{\cntrImg}[2]{\begin{center}\includegraphics[scale=#2]{#1}\end{center}}
\newcommand{\defn}[1]{{\bf #1}}
\let\emptyset\varnothing
\newcommand{\SampS}[0]{$\mathcal{S}$}

\title{Math 3070, Applied Statistics}

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\begin{document}
\begin{frame}[c]
    \begin{beamercolorbox}[rounded=true,wd=\textwidth,center]{title}
        \usebeamerfont{title}\inserttitle
    \end{beamercolorbox}
    \begin{center}
        Section 1\\
        \nl{0.5}
        October 14, 2019
    \end{center}
\end{frame}
\begin{frame}[c]{Lecture Outline, 10/14}
    Section 5.2
    \begin{itemize}
        \item Double Integration
        \item Expected Value
        \item Correlation and Covariance
    \end{itemize}
\end{frame}
\begin{frame}{Double Integration}
    Due to course prerequisites, the following adjustment will be made.
    \begin{itemize}
        \item Brief introduction to double integral will be presented.
        \item Lectures will contain double integrals.
        \item Homework problems that cannot be reduced to single integrals will be graded on completion.
        \item Quiz and exam questions will not require double integration. \textbf{Questions may reference multiple random variables and joint PDFs.} In these question, random variables will usually be assumed to be independent.
    \end{itemize}
\end{frame}
\begin{frame}{Double Integration, Quiz and Exam Questions}
    $X,Y$ are continuous and independent, $f(x,y)=f_X(x)f_Y(y)$.\\
    Events:
    \begin{align*}
        P(a<X<b,c<Y<d) & = P(a<X<b) P(c<Y<d)                           \\
                       & = \int_{a}^b f_X (x) dx \int_{a}^b f_Y (y) dy
    \end{align*}
    Conditional and marginal PDFs, and conditioned events:
    $$ f_{X}(x) = \int_{-\infty}^\infty f_X(x)f_Y(y) dy = f_X(x) \int_{-\infty}^\infty f_Y(y) dy =f_X(x)$$
    $$ f_{Y|X}(y|x) = \frac{f_Y(y)f_X{x}}{f_X{x}} = f_Y(y) $$
    \begin{align*}
        P(a<X<b|c<Y<d) & = \frac{P(a<X<b)P(c<Y<d)}{P(c<Y<d)} \\ & = P(a<X<b)\\
    \end{align*}
\end{frame}
\begin{frame}{Double Integration, Procedure}
    Using double integrals
    \begin{itemize}
        \item[1] Identify the limits of integration from the region of integration. May have to write limit of one variable in terms of the other. PDFs may impose more restrictions. Drawing a picture may help.
        \item[2] Integrate the inner integral. Consider the other variable to be a constant.
        \item[3] Integrate the outer integral.
        \item[\checkmark\checkmark] If any variables are left over, the integral was likely set up incorrectly in step 1. Probabilities are less than 1.
    \end{itemize}
\end{frame}
\begin{frame}{Double Integration, Example}
    $X$ and $Y$ are lifetimes of components and have the following joint PDF. Determine $K$.
    $$
        f(x,y)=
        \begin{cases}
            K(x^2 + y^2), & 0<x,y<1          \\
            0,            & \text{otherwise}
        \end{cases}
    $$
    \uncover<2->{Want: $1= \int_{-\infty}^\infty \int_{-\infty}^\infty f(x,y) dx dy$\\}
    \uncover<3->{Step 1:
        \begin{align*}
            \int_{-\infty}^\infty \int_{-\infty}^\infty f(x,y) dx dy & = \int_{0}^1 \int_{0}^1 K(x^2 + y^2) dx dy \\
        \end{align*}}
\end{frame}
\begin{frame}{Double Integration, Example}
    \uncover<1->{Step 2:
        \begin{align*}
            \int_{0}^1 \int_{0}^1 K(x^2 + y^2) dx dy & = K \int_0^1 \bigg( \frac{x^3}{3} + xy^2 \bigg|_{x=0}^1 \bigg)dy \\
                                                     & = K \int_0^1  \frac{1}{3} + y^2 dy                               \\
        \end{align*}\vskip -2em}
    \uncover<2->{Step 3:
        \begin{align*}
            K \int_0^1  \frac{1}{3} + y^2 dy & = K \bigg( \frac{y}{3} + \frac{y^3}{3} \bigg|_{x=0}^1 \bigg) \\
                                             & = K \bigg( \frac{1}{3} + \frac{1}{3}  \bigg)                 \\
        \end{align*}\vskip -2em}
    \uncover<3->{$$1= K \frac{2}{3} \to K = \frac{3}{2}$$}
\end{frame}
\begin{frame}{Double Integration, Example with Sum}
    Consider $X$ and $Y$ to be independent exponential random variables with parameter $\lambda=1$. Compute the probability that $X+Y<3$.
    \uncover<1->{Independence implies $$
            f(x,y) = \begin{cases}
                e^{-x} e^{-y}, & x,y>0            \\
                0,             & \text{otherwise}
            \end{cases}
        $$}
    \uncover<2->{Step 1: $X<3-Y$ or $Y<3-X$ can both be used as a restriction.
        $$ \int_{-\infty}^\infty \int_{-\infty}^{3-y} f(x,y) dx dy \hskip 0.5em \text{ or } \int_{-\infty}^\infty \int_{-\infty}^{3-x} f(x,y) dy dx$$
        $$ \int_{-\infty}^\infty \int_{-\infty}^{3-y} f(x,y) dx dy = \int_{0}^3 \int_{0}^{3-y} e^{-x} e^{-y} dx dy$$}
\end{frame}
\begin{frame}{Double Integration, Example with Sum}
    \uncover<1->{Step 2:
        \begin{align*}
            \int_{0}^3 \int_{0}^{3-y} e^{-x} e^{-y} dx dy & = \int_{0}^3 e^{-y} \bigg( -e^{-x} \bigg|_{x=0}^{3-y} \bigg) dy \\
                                                          & = \int_{0}^3 e^{-y} (1 - e^{-(3-y)}) dy                         \\
                                                          & = \int_{0}^3 e^{-y} - e^{-3} dy                                 \\
        \end{align*}}
    \uncover<2->{Step 3:
        \begin{align*}
            \int_{0}^3 e^{-y} - e^{-y-3} dy & = -e^{-y} - ye^{-3} \bigg|_{y=0}^3 \\
                                            & = 1 - 4e^{-3} \approx 0.800851727  \\
        \end{align*}}
\end{frame}
\begin{frame}[c]{Expected Value}
    \begin{block}{}
        The expected value of function of discrete random variables is
        $$ E[h(X,Y)] = \sum_{x} \sum_{y} h(x,y) f(x,y) $$
        For continuous random variables the expected value is
        $$ E[h(X,Y)] = \int_{-\infty}^\infty \int_{-\infty}^\infty  h(x,y) f(x,y) dx dy$$
    \end{block}
    The average value of a function of random variables $h$. For example, $h$ could be a length times a length, a random rate times a random time, the sample mean and so on...
\end{frame}

\begin{frame}{Expected Value, Example 1}
    The joint PMF of $X$ and $Y$ are given below. Determine the expected value of $XY$.
    \begin{center}
        \begin{tabular}{l||l|l|l}
            $f(x,y)$ & $y=0$ & $y=1$ & $y=2$ \\ \hline \hline
            $x=0$    & .20   & .10   & .20   \\ \hline
            $x=1$    & .05   & .15   & .30
        \end{tabular}
    \end{center}
    \begin{align*}
        \uncover<2->{E[XY] & = \sum_{x} \sum_{y} xy f(x,y)\\}
        \uncover<3->{      & = (0)(0)0.20 + (0)(1)0.10 + (0)(2)0.20                          \\
                           & + (1)(0)0.05 + (1)(1)0.15 + (1)(2)0.30\\}
        \uncover<4->{      & = 0.75}
    \end{align*}
\end{frame}

\begin{frame}{Expected Value, Example 2}
    The joint PDF of $X$ and $Y$ are given below. Determine the expected value of $X$.
    $$
        f(x,y) =
        \begin{cases}
            \frac{3}{2}(x^2 + y^2), & 0 \leq x,y \leq 1       \\
            0,                      & \text{otherwise}
        \end{cases}
    $$
    \begin{align*}
        \uncover<2->{E[X] & = \int_{-\infty}^\infty \int_{-\infty}^\infty  x f(x,y) dx dy \\}
             \uncover<3->{& = \int_0^1 \int_0^1  x \frac{3}{2}(x^2 + y^2) dx dy = \frac{3}{2} \int_0^1 \int_0^1  x^3 + xy^2 dx dy \\}
             \uncover<4->{& = \frac{3}{2} \int_0^1 \bigg(  \frac{x^4}{4} + \frac{x^2}{2}y^2 \bigg|_{x=0}^1 \bigg) dy = \frac{3}{2} \int_0^1 \frac{1}{4} + \frac{y^2}{2}  dy \\}
             \uncover<5->{& = \frac{3}{2} \bigg( \frac{y}{4} + \frac{y^3}{6} \bigg|_{y=0}^1 \bigg) = \frac{5}{8}                \\}
    \end{align*}
\end{frame}
\begin{frame}{Expected Value and Independence}
    Assume $X$ and $Y$ are independent.
    $$ E[h(X)+g(Y)+c] = E[h(X)] + E[g(Y)] + c $$
    Continous Random Variables (discrete case replaces $\int$ with $\sum$):
    {\small
    \begin{align*}
         & E[X+Y]  = \int \int  (h(x) + g(y) + c) f_X(x) f_Y(y) dx dy                       \\
         & = \int \int  h(x) f_X(x) f_Y(y) dx dy + \int \int  g(y) f_X(x) f_Y(y) dx dy      \\ &+ c \int \int f_X(x)f_Y(y) dx dy\\
         & = \int f_Y(y) \int  h(x) f_X(x)  dx dy + \int g(y) f_Y(y) \int f_X(x)  dx dy + c \\
         & = \int f_Y(y)   E[h(X)] dy + \int g(y) f_Y(y) \int  f_X(x)  dx dy + c            \\
         & = E[h(X)] \int f_Y(y) dy + \int g(y) f_Y(y)  dy + c = E[h(X)] + E[g(Y)] + c      \\
    \end{align*}
    }
\end{frame}
\begin{frame}{Expected Value and Independence}
    Assume $X$ and $Y$ are independent.
    $$ E[h(X)g(Y)] = E[h(X)]E[g(Y)]$$
    Continous Random Variables (discrete case replaces $\int$ with $\sum$):
    {\small
    \begin{align*}
        E[h(X)g(Y)] & = \int \int  h(x)g(y) f_X(x) f_Y(y) dx dy \\
                    & = \int g(y) f_Y(y) \int h(x) f_X(x) dx dy \\
                    & = \int g(y) f_Y(y) E[h(X)] dy             \\
                    & = E[h(X)]\int g(y) f_Y(y)  dy             \\
                    & = E[h(X)]E[g(Y)]                          \\
    \end{align*}
    }
\end{frame}
\begin{frame}{Variance and Independence}
    Assume $X$ and $Y$ are independent.
    $$ Var(aX+bY+c) = a^2Var(X) + b^2Var(Y) $$
    {\small
            \begin{align*}
                E[(aX+bY+c)^2] & = E[a^2X^2 + bY^2 + c^2                     \\
                               & + 2abXY + 2caX + 2bcY]                      \\
                               & = a^2E[X^2] + bE[Y^2] + c^2                 \\
                               & + 2abE[XY] + 2caE[X] + 2bcE[Y]              \\
                E[(aX+bY+c)]^2 & = (aE[X]+bE[Y]+c)^2                         \\
                               & = a^2E[X]^2 + b^2E[Y]^2 + c^2               \\
                               & + 2abE[X]E[Y] + 2caE[X] + 2bcE[Y]           \\
                Var(aX+bY+c)   & = E[(aX+bY+c)^2] - E[(aX+bY+c)]^2           \\
                               & = a^2 (E[X^2]-E[X]^2) + b^2 (E[Y^2]-E[Y]^2) \\
                               & + E[XY] - E[X]E[Y]                          \\
                               & = a^2Var(X) + b^2Var(Y)                     \\
            \end{align*}
        }
\end{frame}
\begin{frame}{Example, Expected Value and Variance of a Binomial}
    Recall that a Binomial random variable can also be defined to be the sum of $n$ independent Bernoulli random variables each with parameter $p$. Use this definition to compute the mean and variance.\\
    \uncover<2->{$X_i \sim Bern(p), \hskip 0.5em 1\ldots n$.\\}
    \uncover<3->{$Y = \sum_{i=1}^n X_i$.\\}
    \uncover<4->{$E[X_i]=p, \hskip 0.5em Var(X_i)=p(1-p)$}
    \uncover<5->{$$E[Y]=E\bigg[\sum_{i=1}^n X_i\bigg] = \sum_{i=1}^nE[ X_i] = \sum_{i=1}^n p = np$$}
    \uncover<6->{$$Var(Y)=Var\bigg(\sum_{i=1}^n X_i\bigg) = \sum_{i=1}^nVar( X_i) = \sum_{i=1}^n p(1-p) = np(1-p)$$}
\end{frame}
\begin{frame}{Example}
    $X$ is a the midterm 1 score of a randomly selected student and $Y$ is their midterm 2 score. Assume that $X$ and $Y$ are independent, $X\sim N(85,10)$ and $Y \sim N(75,14)$. Compute the expected value and variance of the student's average score.
    \uncover<2->{$$\text{student's midterm average} = Z = \frac{X+Y}{2} $$}
    \uncover<3->{$$ E\bigg[ \frac{X+Y}{2}\bigg] = \frac{E[X] + E[Y]}{2} = \frac{85+75}{2} = 80 $$}
    \uncover<4->{$$ Var\bigg( \frac{X+Y}{2}\bigg) = \frac{Var(X)}{4} + \frac{Var(Y)}{4} = \frac{10+14}{4} = 6 $$}
\end{frame}
\begin{frame}{Summary}
    \begin{itemize}
        \item Discrete:$$ E[h(X,Y)] = \sum_{x} \sum_{y} h(x,y) f(x,y) $$
        \item Continuous: $$ E[h(X,Y)] = \int_{-\infty}^\infty \int_{-\infty}^\infty  h(x,y) f(x,y) dx dy$$
        \item Assuming Independence,
              $$E[g(X) + h(Y) + c] = E[g(X)]+E[h(Y)] +c$$
              $$E[XY] = E[X]E[Y]$$
              $$Var(aX + bY + c) = a^2Var(X) + b^2Var(Y)$$
    \end{itemize}
\end{frame}
\begin{frame}{Covariance}
    \begin{block}{}
        Given random variables $X$ and $Y$ with means $\mu_X$ and $\mu_Y$ respectively, their \textbf{covariance} is
        \vskip -1em
        $$Cov(X,Y) = E[(X-\mu_X)(Y-\mu_Y)]$$
        \vskip -0.5em
        Expected change in $X$ times change in $Y$. Simliar to the chapter 1 version.
    \end{block}
    \pause As for variance, there is a shortcut formula for covariance:
    $$Cov(X,Y) = E(XY) - E(X)E(Y)$$
    \vskip -0.5em
    \pause 
    \begin{block}{}
        Proof of the following similar those from expected value section.
        \begin{enumerate}
            \item $Cov(X,X) = V(X)$
            \item $Cov(Y,X) = Cov(X,Y)$
            \item $Cov(cX,Y) = c Cov(X,Y)$
            \item $Cov(X+Y,Z) = Cov(X,Z)+Cov(Y,Z)$
        \end{enumerate}
    \end{block}
\end{frame}
\begin{frame}{Correlation}
    \begin{block}{}
    The \textbf{correlation} between two random variables $X$ and $Y$ is 
    $$\rho = Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{V(X)}\sqrt{V(Y)}} = \frac{Cov(X,Y)}{\sigma_X\sigma_Y}$$
    \end{block}
    Correlation is a measure of how strongly two random variables follow a linear relation. \pause It has several properties:
    \begin{block}{}
    \begin{enumerate}
    \item $-1 \leq \rho \leq 1$
    \item  If $X$ and $Y$ are independent, then $Corr(X,Y)=0$.
    \item Changing the scale of $X$ and/or $Y$ does not affect the correlation, i.e. for any $c\neq 0$,
    $$Corr(cX,Y) = Corr(X,Y) = Corr(X,cY)$$
    \end{enumerate}
    \end{block}
\end{frame}
\begin{frame}{Covariance and Correlation, and Independence }
    Assume $X$ and $Y$ are indpendent.
    \begin{align*}
    Cov(X,Y) & = E[XY] - E[X]E[Y] \\
    & = E[X]E[Y] - E[X]E[Y] = 0\\
    \end{align*}
    \vskip -1.5em
    \begin{itemize}
\item Covariance and Correlation will not appear much for this class. But, correlation and covariance can be incorporated into models. 
\item Specifically, the bivariate Normal distribution is frequently used to model correlated variables.
\item Measures correlation and covariance based on these these ideas appear in data methods such as PCA and Factor analysis.\\
    \end{itemize}
\end{frame}

\begin{frame}{Example}
    \begin{block}{}
    In the bank example, the joint pdf of two random variables $X$ and $Y$ was given by
    $$f(x,y)=\begin{cases}\frac65(x+y^2), & 0\leq x \leq 1, 0\leq y\leq 1 \\ 0, & \text{otherwise}\end{cases}$$
    Find the correlation of $X$ and $Y$.
    \end{block}
    \pause The marginal pdfs of $X$ and $Y$ are, for $0\leq x \leq 1, 0\leq y\leq 1$,
    \begin{align*}
    f_X(x) &= \int_0^1 \frac65(x+y^2)\ dy
    = \frac65\left(xy+\frac{y^3}3\right)\Big\vert_{y=0}^1
    = \frac65\left(x+\frac13\right) \\
    f_Y(y) &= \int_0^1\frac65(x+y^2)\ dx = \frac65\left(\frac{x^2}2+xy^2\right)\Big\vert_{x=0}^1=\frac65\left(\frac12+y^2\right)
    \end{align*}
    \end{frame}
    
    \begin{frame}{Example (continued)}
    From the marginal pdfs $f_X(x)=\frac65(x+\frac13)$ and $f_Y(y)=\frac65(\frac12+y^2)$,
    \begin{align*}
    E(X) &= \int_{-\infty}^\infty xf_X(x)\ dx = \frac65\int_0^1\left(x^2+\frac x3\right)dx = \frac65\left(\frac13+\frac16\right)=\frac35\\
    \uncover<2->{E(Y) &= \int_{-\infty}^\infty yf_Y(y)\ dy = \frac65\int_0^1\left(\frac y2+y^3\right)dy
    =\frac65\left(\frac14+\frac14\right)=\frac35\\}
    \uncover<3->{E(X^2) &=\int_{-\infty}^\infty x^2f_X(x)\ dx = \frac65\int_0^1\left(x^3+\frac {x^2}3\right)dx = \frac65\left(\frac14+\frac19\right)=\frac{13}{30}\\}
    \uncover<4->{E(Y^2) &= \int_{-\infty}^\infty yf_Y(y)\ dy = \frac65\int_0^1\left(\frac {y^2}2+y^4\right)dy
    =\frac65\left(\frac16+\frac15\right)=\frac{11}{25}\\}
    \uncover<5->{V(X) &= E(X^2)-[E(X)]^2 = \frac{13}{30}-\left(\frac35\right)^2=\frac{11}{150} \\}
    \uncover<6->{V(Y) &= E(Y^2)-[E(Y)]^2 = \frac{11}{25}-\left(\frac35\right)^2 = \frac 2{25}}
    \end{align*}
    \end{frame}
    
    \begin{frame}{Example (continued)}
    Finally, from $f(x,y)=\frac65(x+y^2)$ we calculate
    \begin{align*}
    E(XY) &= \int_{-\infty}^\infty\int_{-\infty}^\infty xyf(x,y)\ dx\ dy
    \uncover<2->{= \frac65\int_0^1\int_0^1 (x^2y+xy^3)\ dx\ dy \\}
    \uncover<3->{&= \frac65\int_0^1 \left(\frac{x^3y}3+\frac{x^2y^3}2\right)\Big\vert_{x=0}^1\ dy \\}
    \uncover<4->{&= \frac65\int_0^1\left(\frac y3+\frac{y^3}2\right)\ dy}
    \uncover<5->{= \frac65\left(\frac16+\frac18\right) =\frac7{20}\\[.1cm]}
    \uncover<6->{Cov(X,Y) &= E(XY)-E(X)E(Y) }
    \uncover<7->{= \frac7{20}-\frac35\cdot\frac35 }
    \uncover<8->{= \frac{-1}{100}}
    \end{align*}
    \uncover<9->{Therefore, from $V(X)=11/150$ and $V(Y)=2/25$, 
    \begin{align*}
    \rho &= Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{V(X)}\sqrt{V(Y)}} = \frac{-1/100}
    {\sqrt{11/150}\sqrt{2/25}} \approx -.131
    \end{align*}}
    \end{frame}

\end{document}